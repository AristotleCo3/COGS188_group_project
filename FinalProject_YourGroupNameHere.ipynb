{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINE SWEEPER\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Keyi Yu\n",
    "- Fatima Dong\n",
    "- Kayla Huynh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project aims to build an AI model that solves the game Minesweeper using reinforcement learning algorithms. Minesweeper is a logic based game where the goal is to uncover all non-mine cells while avoiding mines. The game environment is a grid where each cell can either contain a mine or be safe. The numbers on the revealed cells tell you the number of adjacent mines. If you accidentally uncover the mine cell, then the game will end. The dataset is generated from a Python-based Minesweeper implementation, consisting of game states, player actions, and board configurations.\n",
    "The mindsweeper solver will operate as a rational agent by optimizing its performance uncovering safe cells, in the stochastic environment. We will be using search algorithms and heuristic strategies to effectively solve the problem. To evaluate performance, we will use win rate and average game duration as key metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Minesweeper is a single-player game where players must uncover all safe tiles without clicking on a mine! Safe squares provide numerical clues indicating how many mines are within a one-tile radius. The game features three official board sizes: Beginner (8×8 grid with 10 mines), Intermediate (16×16 grid with 40 mines), and Expert (16×30 grid with 90 mines). Clicking on a mine ends the game immediately, while selecting a safe tile may reveal numbers or automatically clear nearby squares. The challenge relies on using logic while simultaneously minimizing the risk of guessing. To win, all non-mine squares must be uncovered, leaving only the mines flagged.\n",
    "<br> <br>\n",
    "According to Kaye, “the Minesweeper problem is NP-Complete”, meaning that it is highly unlikely that there is an efficient algorithm that can solve it and that it is just as difficult as any other NP-Complete problem (like the traveling salesman problem)[1]. In other words, there is no known algorithm that can solve Minesweeper in polynomial time. Despite that some parts of the board can be determined through logical reasoning, some configurations require probabilistic guessing, making Minesweeper a constraint satisfaction problem[2].\n",
    "<br> <br>\n",
    "Given these papers, reinforcement learning will be explored in this project. There exists research, where Monte Carlo Simulation was used to solve the Minesweeper problem[3]. Another study showed that a mix of optimal heuristics proved to solve the problem with more efficiency. Such heuristics included: targeting the corners of the grid based on a previous study[2] due to the fact that the density of a mine in a corner is lower than any other tiles, maximizing the probability of revealing at least one safe-block in one move, and maximizing the expected number of safe blocks in the next move[4]. This greedy heuristic algorithm was named PSEQ. In a different study, double deep-Q-network was applied to the problem[5]. For this project, we will focus on implementing the Q-learning algorithm and additional heuristic(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Mindsweeper is a puzzle game where it requires players to uncover safe cells while avoiding the mine cells using the adjacent numerical clues. The goal of the game is to uncover the whole entire board without touching a mine. This game requires different strategies and sometimes risk taking when you run out of clues. You have to be able to make the most optimal decision within the stochastic environment where you don’t know where the mines are. The problem we are addressing is developing an AI-based Minesweeper solver that will be the most efficient when in play, using search algorithms and heuristic strategies. This problem is quantifiable since we are able to perform probability calculations and logical inference. It is also measurable since we can calculate the performance based on win rates and average game durations. Lastly, it is replicable since each time you play it is a different game board, allowing us to consistently use our algorithm since the rules do not change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data for this project will be generated using a Pygame-based Minesweeper implementation (https://pypi.org/project/pygame-minesweeper/ ). The dataset consists of game states, score ranking, and the final outcome, whether a win or loss. This implementation has various board configurations, such as Basic (10x10 grid with 10 mines), Intermediate (16x16 grid with 40 mines), Expert (6x30 grid with 99 mines), and Custom (users can define the number of rows, columns, and mines). Each board state is represented as a 2D grid where cells can be hidden. They can be revealed with a number indicating the count of adjacent mines, or flagged as a potential mine by the solver. The data collection process involves the solver interacting with the Minesweeper game by tracking wins or loses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The goal of this project is to develop two agents, TD Learning and DQN, to solve Minesweeper, a challenging single-player puzzle. The agent will learn to uncover safe tiles while avoiding mines, using exploration and exploitation to optimize its strategy.\n",
    "\n",
    "## TD-Learning\n",
    "We intend to implement TD-Learning, a reinforcement learning that combines both dynamic programming and Markov Carlo Methods. \n",
    "The TD Learning Agent uses an epsilon-greedy policy to choose its actions. The TD Learning Equation is:\n",
    "$$V(s)←V(s)+ \\alpha (R+ \\gamma V(s′)−V(s))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- V(s) is the value function of the current state,\n",
    "- R is the immediate reward,\n",
    "- Gamma is the discount factor,\n",
    "- V(s′) is the value function of the next state,\n",
    "- Alpha is the learning rate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DQN\n",
    "\n",
    "We will implement the Deep Q-Networks (DQN), a reinforcement learning algorithm that uses a neural network to approximate Q-values for state-action pairs for solving the Mineweeper game. DQN was chosen because it can handle large state spaces and delayed reward to allow the agent to learn the optimal strategy over time. We incorporated experience replay,  a target network, and exploration-exploitation trade-off to stabilize training and also prevent catastrophic forgetting. Additionally, we designed a reward function that encourages the agent to prioritize safer moves (e.g. opening tiles near revealed low-numbered tiles and prioritizing corners). We believe that by adjusting the hyperparameters and optimizing the exploration and reward strategies, DQN was able to learn and significantly improve the performance which ultimately achieved a high win rate.\n",
    "\n",
    "### Additional Heuristic\n",
    "To improve efficiency, heuristics such as corner targeting (prioritizing corners) will be integrated into the learning process and targeting adjacent tiles for revealed tiles with a value of 1.\n",
    "\n",
    "## Benchmark Model\n",
    "For evaluation, the random agent will serve as a baseline model, selecting actions randomly from the available tiles. The performance will be compared in terms of win rate, time to solve, and number of moves, with both of the agents expected to outperform the random agent in all metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "### **Episode Reward**\n",
    "The **total reward** accumulated by the agent **in a single episode**. It indicates how well the agent is optimizing the reward function during training.  \n",
    "A higher total reward suggests that the agent is **making better decisions** and uncovering more tiles without hitting a mine.  \n",
    "\n",
    "$$\n",
    "R_{total} = \\sum_{t=1}^{T} r_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R_{total}$ = Total reward in an episode  \n",
    "- $r_t$ = Reward at time step $t$\n",
    "- $T$ = Length of the episode  \n",
    "\n",
    "---\n",
    "\n",
    "### **Win Rate**\n",
    "The **percentage of games won** by the agent over a set number of episodes. It reflects the **agent’s ability to consistently win games** as training progresses.  \n",
    "\n",
    "$$\n",
    "\\text{Win Rate} = \\left( \\frac{\\text{Number of Wins}}{\\text{Total Episodes}} \\right) \\times 100\n",
    "$$\n",
    "\n",
    "where:\n",
    "- **Number of Wins** = Games successfully completed  \n",
    "- **Total Episodes** = Total training episodes  \n",
    "\n",
    "---\n",
    "\n",
    "### **Episode Length**\n",
    "The **number of steps the agent takes before the game ends** (either by winning or hitting a mine).  \n",
    "A **longer episode length** suggests that the agent is **surviving longer and making more strategic moves**.  \n",
    "\n",
    "$$\n",
    "L_{episode} = T\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L_{episode}$ = Episode length  \n",
    "- $T$ = Total number of moves taken in the episode  \n",
    "\n",
    "---\n",
    "\n",
    "### **Exploration Rate (Epsilon Decay)**\n",
    "This metric tracks **how much the agent explores versus exploits learned knowledge**.  \n",
    "Epsilon **starts high** (random exploration) and **decays over time**, leading to **more exploitation of learned strategies**.  \n",
    "\n",
    "$$\n",
    "\\epsilon_{t+1} = \\max(\\epsilon_{min}, \\epsilon_t \\times \\epsilon_{decay})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon_t$ = Epsilon at time step $t$  \n",
    "- $epsilon_{decay}$ = Decay rate (e.g., 0.997)  \n",
    "- $\\epsilon_{min}$ = Minimum exploration rate  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tiles Revealed Per Episode**\n",
    "The **number of safe tiles uncovered** before the game ends.  \n",
    "A **higher number of revealed tiles** suggests that the agent **is effectively learning to clear safe areas while avoiding mines**.  \n",
    "\n",
    "$$\n",
    "\\text{Tiles Revealed} = \\sum_{i,j} \\mathbf{1} [\\text{tile}_{i,j} \\neq \\text{unopened}]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{1}$ is an indicator function that **counts opened tiles**  \n",
    "- $i, j$ are tile positions on the Minesweeper grid  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "## Random Agent\n",
    "\n",
    "### Random Agent (Baseline Model) Results and Visualizations:\n",
    "The Random Agent was tested over 1,000 episodes in Minesweeper as a baseline model to evaluate performance rates.  The agent randomly selects tiles making it a more stochastic approach to the game. After running multiple simulations, performance metrics were recorded, such as win rate, average moves per game, and total reward over time. The win rate was extremely low, averaging around 0%, confirming that random selection is not an effective strategy for Minesweeper. The majority of games ended within a small number of moves due to clicking on a mine, immediately ending the game. On average, the agent survived for 4 moves before encountering a mine. The reward system showed that rewards fluctuated randomly with no visible trend of improvement. Since the agent does not learn from past games, the reward distribution remained disorderly, proving that random guessing does not contribute to better performance over time. The reward system penalized mine hits with -20 points and awarded +100 points for winning a game, but wins were so rare that they had little impact on overall performance. The number of moves per game varied significantly, as seen in the move count distribution plot. Some games ended immediately, while others lasted longer by pure chance. However, since there was no learning mechanism in place, the agent did not develop any pattern or strategy to increase survival. The lack of optimization further supports the need for a more intelligent approach, such as TD-Learning or DQN, to solve Minesweeper. \n",
    "\n",
    "![IMG_1284](imgs/IMG_1284.jpeg)\n",
    "\n",
    "![IMG_9951](imgs/IMG_9951.jpeg)\n",
    "\n",
    "Given that the win rate is 0%, this means that the random agent didn’t win a single game out of the 1,000 episodes, meaning the agent wasn’t able to uncover the safe cells before clicking on a mine. \n",
    "The average moves were around 4, so the agent was able to survive the game with 4 moves before ending the game. This implies that random guessing is highly ineffective compared to strategic moves in TD-Learning and DQN. \n",
    "\n",
    "\n",
    "The average reward was around -2.72, since the reward system penalizes hitting mines at -20. Having a negative average reward further confirms that the agent has more losses than wins. Even though the agent receives occasional positive rewards, it was not enough to outweigh the consequences.\n",
    "\n",
    "\n",
    "## TD Learning\n",
    "\n",
    "The code snippet below illustrates the training of TD Learning.\n",
    "\n",
    "![tdlearning](imgs/tdlearning.png)\n",
    "\n",
    "### TD Hyperparameters\n",
    "The hyperparameters were chosen based on the best average reward for the entire episode.\n",
    "![hp](imgs/hyperparams_td.png)\n",
    "\n",
    "In this case the best hyperparameters for alpha, gamma, and epsilon respectively are: (0.3, 0.97, 0.1)\n",
    "\n",
    "### TD Rewards\n",
    "![re_td](imgs/rewards_td.png)\n",
    "\n",
    "In this implementation for TD Learning, a couple of heuristics were considered, where choosing a corner is prioritized and choosing adjacent tiles to “1” valued tiles. The second heuristic was considered as the number value of the tile indicating the number of mines nearby. The chances of revealing a mine are considerably lower when it is a small value like 1, compared to a value of 8. \n",
    "\n",
    "\n",
    "Based on the above plots, it can be concluded that TD Learning does not necessarily perform outstandingly as the Win Rate is 0%. In other words, the agent is unable to complete the game without landing on a mine for all 1000 episodes. The average number of step is 28.98 for TD Learning with the specific parameters. However, the average reward accumulated here is 65.367, which indicates some learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks (DQN)\n",
    "\n",
    "The code snippet below illustrates the training of DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from minesweeper_dqn import MinesweeperEnv, DQNAgent\n",
    "\n",
    "def train_agent(num_episodes=1000, max_steps=100):\n",
    "   env = MinesweeperEnv(rows=10, cols=10, mines=30)\n",
    "   agent = DQNAgent(env)\n",
    "  \n",
    "   episode_rewards = []\n",
    "   win_rates = []\n",
    "   episode_lengths = [] \n",
    "   exploration_rates = [] \n",
    "   tiles_revealed = [] \n",
    "   win_count = 0 \n",
    "\n",
    "   for episode in range(num_episodes):\n",
    "       state = env.reset()\n",
    "       total_reward = 0\n",
    "       done = False\n",
    "       steps = 0\n",
    "\n",
    "       initial_revealed = sum(1 for row in env.board._tiles for tile in row if tile.type != \"t\")\n",
    "\n",
    "       for step in range(max_steps):\n",
    "           action = agent.choose_action(state)\n",
    "           next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "           agent.remember(state, action, reward, next_state, done)\n",
    "           agent.replay()\n",
    "          \n",
    "           state = next_state\n",
    "           total_reward += reward\n",
    "           steps += 1 \n",
    "\n",
    "           if done:\n",
    "               if env.board.is_game_finished:\n",
    "                   win_count += 1\n",
    "               break\n",
    "\n",
    "       final_revealed = sum(1 for row in env.board._tiles for tile in row if tile.type != \"t\")\n",
    "       revealed_tiles = final_revealed - initial_revealed \n",
    "      \n",
    "       episode_rewards.append(total_reward)\n",
    "       win_rates.append(win_count / (episode + 1) * 100) \n",
    "\n",
    "       episode_lengths.append(steps) \n",
    "       exploration_rates.append(agent.epsilon) \n",
    "       tiles_revealed.append(revealed_tiles) \n",
    "\n",
    "       print(f\"Episode {episode+1}/{num_episodes} - Reward: {total_reward:.2f}, Win Rate: {win_rates[-1]:.3f}%, Steps: {steps}, Tiles Revealed: {revealed_tiles}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "   return episode_rewards, win_rates, episode_lengths, exploration_rates, tiles_revealed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Model Hyperparameters Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "   def __init__(self, env, lr=0.0003, gamma=0.98, epsilon=1.0, epsilon_decay=0.997, epsilon_min=0.01, batch_size=64):\n",
    "       self.env = env\n",
    "       self.state_size = env.get_state().size\n",
    "       self.action_size = env.get_action_space_size()\n",
    "       self.memory = deque(maxlen=50000) \n",
    "       self.gamma = gamma\n",
    "       self.epsilon = epsilon\n",
    "       self.epsilon_decay = epsilon_decay\n",
    "       self.epsilon_min = epsilon_min\n",
    "       self.batch_size = batch_size\n",
    "       self.tau = 0.01 \n",
    "\n",
    "       self.model = QNetwork(self.state_size, self.action_size) \n",
    "       self.target_model = QNetwork(self.state_size, self.action_size) \n",
    "\n",
    "       self.target_model.load_state_dict(self.model.state_dict())\n",
    "       self.target_model.eval() \n",
    "\n",
    "       self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "       self.criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the **Deep Q-Network (DQN)** agent depends heavily on the choice of hyperparameters, as they control the agent’s ability to **explore, learn, and converge** toward an optimal policy. Through iterative testing and tuning, we selected the following hyperparameters to ensure a balance between **exploration, learning stability, and efficient training**.\n",
    "\n",
    "### **Learning Rate (lr=0.0003 for training, 0.0001 for optimization)**\n",
    "The **learning rate** determines how quickly the model updates its weights during training.  \n",
    "- A **higher learning rate** allows the model to learn faster but risks instability.  \n",
    "- A **lower learning rate** ensures more stable learning but may slow down convergence.  \n",
    "- We set the **main learning rate to 0.0003**, which balances fast learning and stability.  \n",
    "- The **optimizer’s learning rate is set to 0.0001** to ensure smoother updates and prevent drastic weight changes that could disrupt training.\n",
    "\n",
    "### **Discount Factor (gamma=0.98)**\n",
    "The **discount factor (gamma)** defines how much the agent prioritizes **future rewards over immediate ones**.  \n",
    "- A **higher gamma (close to 1)** encourages long-term planning.  \n",
    "- A **lower gamma** makes the agent more focused on immediate rewards.  \n",
    "- Since **Minesweeper** requires careful planning and strategic moves, we set **gamma to 0.98** to ensure the agent prioritizes **long-term success while still considering short-term gains**.\n",
    "\n",
    "### **Epsilon-Greedy Exploration Strategy (epsilon=1.0, epsilon_decay=0.997, epsilon_min=0.01)**\n",
    "To balance **exploration and exploitation**, we use an **epsilon-greedy strategy**.  \n",
    "- The initial **epsilon = 1.0** allows the agent to explore freely at the beginning of training.  \n",
    "- Over time, **epsilon decays at a rate of 0.997** to shift from **exploration to exploitation**, enabling the agent to leverage its learned policy.  \n",
    "- The **minimum epsilon = 0.01** ensures the agent **never stops exploring completely**, preventing it from getting stuck in a suboptimal strategy.  \n",
    "- This combination ensures the agent **explores effectively early on** while progressively learning to make better decisions.\n",
    "\n",
    "### **Experience Replay Buffer (self.memory = deque(maxlen=50000))**\n",
    "The **experience replay buffer** stabilizes training by storing past experiences `(state, action, reward, next state)`.  \n",
    "- The buffer allows the agent to **learn from a diverse set of experiences** rather than just recent ones.  \n",
    "- We **increased the buffer size from 10,000 to 50,000** to ensure the agent **retains a wider variety of past experiences**, which improves **generalization**.  \n",
    "- This prevents the network from becoming too dependent on short-term variations in the environment, which leads to more stable training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training Results and Visualization\n",
    "\n",
    "### Overall Performance (1000 Episodes)\n",
    "\n",
    "The **Deep Q-Network (DQN)** agent trained to play Minesweeper demonstrated **significant learning over time**.  \n",
    "The primary objective was to **maximize the number of revealed tiles** while **avoiding bombs**, ultimately **achieving a win** by uncovering all non-mine tiles.  \n",
    "\n",
    "- Over the course of training, the agent **progressively improved its decision-making**, with the **win rate increasing** and the **total rewards becoming more stable**.  \n",
    "- However, there were **fluctuations in performance**, indicating **potential areas for improvement**, particularly in **exploration strategies and reward structuring**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Training Rewards Over Episodes**\n",
    "\n",
    "![Training Rewards Graph](imgs/training_rewards.png)  \n",
    "\n",
    "This graph tracks the **total reward earned** by the agent in each episode.  \n",
    "\n",
    "- The **raw episode rewards (blue)** show **high variability early in training**, indicating that the agent was initially **taking random actions** with no clear strategy.  \n",
    "- However, as training progressed, the **reward generally increased**, which shows that the agent **learned to make better moves**.  \n",
    "- The **smoothed reward curve (red)** provides a clearer view of the trend, showing that the agent **learns quickly, forgets temporarily, and then stabilizes** for receiving high rewards.  \n",
    "- The periods where the **reward drops** might indicate **over-exploration or suboptimal state-action evaluations**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Win Rate Over Episodes**\n",
    "\n",
    "![Win Rate Graph](imgs/win_rate.png)\n",
    "\n",
    "This plot represents the **percentage of games won** as training progresses.  \n",
    "\n",
    "- Initially, the agent had a **near-zero win rate** due to **random exploration**.  \n",
    "- After around **200 episodes**, there was a **sharp increase** in the win rate to **25%**, indicating that the agent **began recognizing patterns and avoiding risky moves**.  \n",
    "- The **win rate continued to improve steadily**, reaching over **70%** by the end of training.  \n",
    "- This suggests that the agent **developed a reliable strategy** for playing Minesweeper but still **experiences occasional losses**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Episode Length Over Time**\n",
    "\n",
    "![Episode Length Graph](imgs/epsilon_length.png)\n",
    "\n",
    "The **episode length metric** represents **how long the agent survived** before either **winning or hitting a mine**.  \n",
    "\n",
    "- **Early in training**, episode length was **highly variable**, with many **short games** due to random moves leading to mines.  \n",
    "- As training progressed, the **episode length increased**, indicating that the agent **learned to survive longer**.  \n",
    "- Eventually, episode length **stabilized**, meaning the agent reached a point where it **efficiently maximized tile exploration** without unnecessary steps.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Exploration Rate (Epsilon Decay) Over Time**\n",
    "\n",
    "![Epsilon Decay Graph](imgs/exploration_rate.png)\n",
    "\n",
    "This graph tracks **how the exploration rate (ε) decreased** over time.  \n",
    "\n",
    "- Initially, the agent relied heavily on **random moves** (**high epsilon**), which explores the environment with no clear strategy.  \n",
    "- As training progressed, **epsilon decreased**, leading to **more exploitation** of learned strategies instead of random exploration.  \n",
    "- By the later stages of training, **epsilon approached 0**, meaning that the agent **primarily relied on its learned policy** rather than random exploration.  \n",
    "- This behavior is **expected in reinforcement learning**, as the model **shifts from exploration to optimal decision-making**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tiles Revealed Per Episode**\n",
    "\n",
    "![Tiles Revealed Graph](imgs/tiles_revealed.png)\n",
    "\n",
    "This graph tracks **the number of tiles** the agent **successfully uncovered** in each episode.  \n",
    "\n",
    "- **Initially, the agent revealed very few tiles** before hitting a mine, losing the game very quickly.  \n",
    "- Over time, as it **learned to identify safer moves**, the number of revealed tiles **increased and stabilized around 70 tiles**.  \n",
    "- This indicates that the agent **successfully reveals all safe tiles and wins the game** (since the **board is 10x10 with 30 mines**).  \n",
    "- The **fluctuations** suggest that while the agent has improved, **there are still occasional episodes where it performs poorly**, likely due to **suboptimal moves or unlucky mine placements**.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Based on our plots, it is evident that DQN significantly outperformed both TD Learning and Random agents in completing the Minesweeper game. One key reason for this is DQN’s use of experience replay. It stores past experiences as tuples of (state, action, reward, next state) in a dataset called replay memory and randomly samples from this memory during training. As a result, DQN demonstrated improved performance with each consecutive episode, as shown in the Win Rate Over Episodes plot.\n",
    "\n",
    "Additionally, experience replay enabled DQN to maintain a balance between exploration and exploitation. The Exploration Rate plot illustrates how the exploration rate gradually decreased over time, allowing the agent to shift toward exploitation strategies that optimized its chances of winning. This trend is consistent across all plots, demonstrating how DQN’s epsilon-greedy strategy facilitated steady learning and ultimately led to consistent success in Minesweeper.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "One limitation to the random agent is the fact that it does not learn from previous games which isn’t good since there is no room for improvement. The random agents goes into the game blindly with no strategy, making moves based off of luck. Another limitation is the variance in the game. Since the random agent depends on luck, one game can be very short by clicking on the mine within one or two moves. On the other hand there could be a more successful game where the random agent gets really lucky with its moves and avoids the mine for longer.   \n",
    "\n",
    "A key limitation of the TD Learning agent in Minesweeper is that the board environment is partially observable. Since the agent does not have full visibility of the game state, it struggles to accurately evaluate its position and make optimal decisions. TD Learning updates its value estimates based on immediate rewards, which is not ideal in Minesweeper, as success requires a multi-step strategy rather than short-term rewards. Additionally, Minesweeper presents a unique challenge: stepping on a mine results in an automatic loss. This makes it difficult for TD Learning to effectively balance exploration and exploitation.\n",
    "\n",
    "One of the main limitations of the current DQN implementation is the instability in the learning curve, where there are sudden drops in rewards even after periods of learning. This suggests that the agent occasionally fails to generalize its learned strategy and potentially gets trapped in suboptimal policies or overfitting to specific patterns. Additionally, since Minesweeper has inherent uncertainty (e.g., forced guessing when no safe move is available), the agent may struggle in situations where logical deduction is limited, and lead to inconsistent performance across episodes.\n",
    "   \n",
    "\n",
    "\n",
    "### Future work\n",
    "To improve the stability of learning and address the sudden drops in rewards, future work could explore techniques like prioritized experience replay, which ensures the agent learns more effectively from critical experiences rather than sampling past experiences uniformly. Another promising direction is integrating probabilistic reasoning into the model, where the agent can compute the likelihood of a tile being a mine based on nearby revealed numbers. This could be achieved by either modifying the state representation to include probability estimates or by incorporating a hybrid approach that combines reinforcement learning with rule-based decision-making. Ultimately, the goal is to develop a more stable and intelligent Minesweeper solver that can handle uncertainty more effectively while maintaining consistent performance improvements over time.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Minesweeper itself is a pretty ethical game. We will not be using any data that would intrude personal privacy. Our solver will not conflict with any other player’s experience since this is only a single player game. The only concerns that we could possibly have is with our AI methods, where our solution would defeat the purpose of the game and be too overpowered. To prevent any issues, we will make sure to state all the methods that we use and be transparent as possible.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our project demonstrates the significant benefits of utilizing Deep Q-Network (DQN) for training the optimal solution for the Mineaweeper compared to the temporal difference learning (TD learning). Unlike the other two models, DQN uses its neural network and experience replay to generalize across board states and finally find the optimal solution. This project shows the power of deep reinforcement learning in solving complex decision-making problems like Minesweeper. However, our model still needs further improvement to stabilize training and enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name= \"kayenote\"></a>1.[^](#kaye): Kaye, R. (2000). \"Minesweeper is NP-Complete.\" Retrieved February 13, 2025. From https://academic.timwylie.com/17CSCI4341/minesweeper_kay.pdf <br> \n",
    "<a name= \"studholmenote\"></a>2.[^](#studholme):Studholme, C. (2000). Minesweeper as a Constraint Satisfaction Problem. Unpublished project report. Retrieved February 13, 2025. From https://www.cs.toronto.edu/~cvs/minesweeper/minesweeper.pdf<br> \n",
    "<a name= \"qingnote\"></a>3.[^](#qing):Qing, Y. et al. (2020). Critical exponents and the universality class of a minesweeper percolation model. International Journal of Modern Physics C, Volume 31, Issue 9, id. 2050129. Retrieved February 13, 2025. From https://ui.adsabs.harvard.edu/abs/2020IJMPC..3150129Q/abstract DOI: 10.1142/S0129183120501296 <br> \n",
    "<a name= \"tunote\"></a>4.[^](#tu):Tu, J. (n.d.). Exploring Efficient Strategies for Minesweeper. Retrieved February 13, 2025. From https://cdn.aaai.org/ocs/ws/ws0294/15091-68459-1-PB.pdf .<br> \n",
    "<a name= \"smuldersnote\"></a>5.[^](#smulders): Smulders, B.G.J.P.( 25 Jun 2023) Optimizing minesweeper and its hexagonal variant with deep reinforcement learning. Retrieved February 13, 2025. From https://pure.tue.nl/ws/portalfiles/portal/307404348/Thesis_BDS_Juli_G._Smulders.pdf  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
