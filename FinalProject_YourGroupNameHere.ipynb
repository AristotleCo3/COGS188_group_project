{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINE SWEEPER\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Keyi Yu\n",
    "- Fatima Dong\n",
    "- Kayla Huynh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project aims to build an AI model that solves the game Minesweeper using reinforcement learning algorithms. Minesweeper is a logic based game where the goal is to uncover all non-mine cells while avoiding mines. The game environment is a grid where each cell can either contain a mine or be safe. The numbers on the revealed cells tell you the number of adjacent mines. If you accidentally uncover the mine cell, then the game will end. The dataset is generated from a Python-based Minesweeper implementation, consisting of game states, player actions, and board configurations.\n",
    "The mindsweeper solver will operate as a rational agent by optimizing its performance uncovering safe cells, in the stochastic environment. We will be using search algorithms and heuristic strategies to effectively solve the problem. To evaluate performance, we will use win rate and average game duration as key metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Minesweeper is a single-player game where players must uncover all safe tiles without clicking on a mine! Safe squares provide numerical clues indicating how many mines are within a one-tile radius. The game features three official board sizes: Beginner (8×8 grid with 10 mines), Intermediate (16×16 grid with 40 mines), and Expert (16×30 grid with 90 mines). Clicking on a mine ends the game immediately, while selecting a safe tile may reveal numbers or automatically clear nearby squares. The challenge relies on using logic while simultaneously minimizing the risk of guessing. To win, all non-mine squares must be uncovered, leaving only the mines flagged.\n",
    "<br> <br>\n",
    "According to Kaye, “the Minesweeper problem is NP-Complete”, meaning that it is highly unlikely that there is an efficient algorithm that can solve it and that it is just as difficult as any other NP-Complete problem (like the traveling salesman problem)[1]. In other words, there is no known algorithm that can solve Minesweeper in polynomial time. Despite that some parts of the board can be determined through logical reasoning, some configurations require probabilistic guessing, making Minesweeper a constraint satisfaction problem[2].\n",
    "<br> <br>\n",
    "Given these papers, reinforcement learning will be explored in this project. There exists research, where Monte Carlo Simulation was used to solve the Minesweeper problem[3]. Another study showed that a mix of optimal heuristics proved to solve the problem with more efficiency. Such heuristics included: targeting the corners of the grid based on a previous study[2] due to the fact that the density of a mine in a corner is lower than any other tiles, maximizing the probability of revealing at least one safe-block in one move, and maximizing the expected number of safe blocks in the next move[4]. This greedy heuristic algorithm was named PSEQ. In a different study, double deep-Q-network was applied to the problem[5]. For this project, we will focus on implementing the Q-learning algorithm and additional heuristic(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Mindsweeper is a puzzle game where it requires players to uncover safe cells while avoiding the mine cells using the adjacent numerical clues. The goal of the game is to uncover the whole entire board without touching a mine. This game requires different strategies and sometimes risk taking when you run out of clues. You have to be able to make the most optimal decision within the stochastic environment where you don’t know where the mines are. The problem we are addressing is developing an AI-based Minesweeper solver that will be the most efficient when in play, using search algorithms and heuristic strategies. This problem is quantifiable since we are able to perform probability calculations and logical inference. It is also measurable since we can calculate the performance based on win rates and average game durations. Lastly, it is replicable since each time you play it is a different game board, allowing us to consistently use our algorithm since the rules do not change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data for this project will be generated using a Pygame-based Minesweeper implementation (https://pypi.org/project/pygame-minesweeper/ ). The dataset consists of game states, score ranking, and the final outcome, whether a win or loss. This implementation has various board configurations, such as Basic (10x10 grid with 10 mines), Intermediate (16x16 grid with 40 mines), Expert (6x30 grid with 99 mines), and Custom (users can define the number of rows, columns, and mines). Each board state is represented as a 2D grid where cells can be hidden. They can be revealed with a number indicating the count of adjacent mines, or flagged as a potential mine by the solver. The data collection process involves the solver interacting with the Minesweeper game by tracking wins or loses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The goal of this project is to develop two agents, TD Learning and DQN, to solve Minesweeper, a challenging single-player puzzle. The agent will learn to uncover safe tiles while avoiding mines, using exploration and exploitation to optimize its strategy.\n",
    "\n",
    "## TD-Learning\n",
    "We intend to implement TD-Learning, a reinforcement learning that combines both dynamic programming and Markov Carlo Methods. \n",
    "The TD Learning Agent uses an epsilon-greedy policy to choose its actions. The TD Learning Equation is:\n",
    "$$V(s)←V(s)+ \\alpha (R+ \\gamma V(s′)−V(s))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- V(s) is the value function of the current state,\n",
    "- R is the immediate reward,\n",
    "- Gamma is the discount factor,\n",
    "- V(s′) is the value function of the next state,\n",
    "- Alpha is the learning rate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DQN\n",
    "\n",
    "We will implement the Deep Q-Networks (DQN), a reinforcement learning algorithm that uses a neural network to approximate Q-values for state-action pairs for solving the Mineweeper game. DQN was chosen because it can handle large state spaces and delayed reward to allow the agent to learn the optimal strategy over time. We incorporated experience replay,  a target network, and exploration-exploitation trade-off to stabilize training and also prevent catastrophic forgetting. Additionally, we designed a reward function that encourages the agent to prioritize safer moves (e.g. opening tiles near revealed low-numbered tiles and prioritizing corners). We believe that by adjusting the hyperparameters and optimizing the exploration and reward strategies, DQN was able to learn and significantly improve the performance which ultimately achieved a high win rate.\n",
    "\n",
    "### Additional Heuristic\n",
    "To improve efficiency, heuristics such as corner targeting (prioritizing corners) will be integrated into the learning process and targeting adjacent tiles for revealed tiles with a value of 1.\n",
    "\n",
    "## Benchmark Model\n",
    "For evaluation, the random agent will serve as a baseline model, selecting actions randomly from the available tiles. The performance will be compared in terms of win rate, time to solve, and number of moves, with both of the agents expected to outperform the random agent in all metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the evaluation of the performance of our Minesweeper solver, we plan to utilize the win rate and average game duration as our evaluation metrics. For the win rate, it is calculated as the percentage of games won out of the total games played. Mathematically, this is represented as:\n",
    "\n",
    "Win Rate = (Number of Games Won/ Total Games Played) * 100\n",
    "\n",
    "A higher win rate indicates better performance in solving Minesweeper boards. For the average game duration, we plan to measure how long it takes for the solver to complete a game which is already implemented in the Pygame Minesweeper. Shorter game duration for a successful game indicates better performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "## Random Agent\n",
    "\n",
    "### Random Agent (Baseline Model) Results and Visualizations:\n",
    "The Random Agent was tested over 1,000 episodes in Minesweeper as a baseline model to evaluate performance rates.  The agent randomly selects tiles making it a more stochastic approach to the game. After running multiple simulations, performance metrics were recorded, such as win rate, average moves per game, and total reward over time. The win rate was extremely low, averaging around 0%, confirming that random selection is not an effective strategy for Minesweeper. The majority of games ended within a small number of moves due to clicking on a mine, immediately ending the game. On average, the agent survived for 4 moves before encountering a mine. The reward system showed that rewards fluctuated randomly with no visible trend of improvement. Since the agent does not learn from past games, the reward distribution remained disorderly, proving that random guessing does not contribute to better performance over time. The reward system penalized mine hits with -20 points and awarded +100 points for winning a game, but wins were so rare that they had little impact on overall performance. The number of moves per game varied significantly, as seen in the move count distribution plot. Some games ended immediately, while others lasted longer by pure chance. However, since there was no learning mechanism in place, the agent did not develop any pattern or strategy to increase survival. The lack of optimization further supports the need for a more intelligent approach, such as TD-Learning or DQN, to solve Minesweeper. \n",
    "\n",
    "![IMG_1284](imgs/IMG_1284.jpeg)\n",
    "\n",
    "![IMG_9951](imgs/IMG_9951.jpeg)\n",
    "\n",
    "Given that the win rate is 0%, this means that the random agent didn’t win a single game out of the 1,000 episodes, meaning the agent wasn’t able to uncover the safe cells before clicking on a mine. \n",
    "The average moves were around 4, so the agent was able to survive the game with 4 moves before ending the game. This implies that random guessing is highly ineffective compared to strategic moves in TD-Learning and DQN. \n",
    "\n",
    "\n",
    "The average reward was around -2.72, since the reward system penalizes hitting mines at -20. Having a negative average reward further confirms that the agent has more losses than wins. Even though the agent receives occasional positive rewards, it was not enough to outweigh the consequences.\n",
    "\n",
    "\n",
    "## TD Learning\n",
    "\n",
    "The code snippet below illustrates the training of TD Learning.\n",
    "\n",
    "![tdlearning](imgs/tdlearning.png)\n",
    "\n",
    "### TD Hyperparameters\n",
    "The hyperparameters were chosen based on the best average reward for the entire episode.\n",
    "![hp](imgs/hyperparams_td.png)\n",
    "\n",
    "In this case the best hyperparameters for alpha, gamma, and epsilon respectively are: (0.3, 0.97, 0.1)\n",
    "\n",
    "### TD Rewards\n",
    "![re_td](imgs/rewards_td.png)\n",
    "\n",
    "In this implementation for TD Learning, a couple of heuristics were considered, where choosing a corner is prioritized and choosing adjacent tiles to “1” valued tiles. The second heuristic was considered as the number value of the tile indicating the number of mines nearby. The chances of revealing a mine are considerably lower when it is a small value like 1, compared to a value of 8. \n",
    "\n",
    "\n",
    "Based on the above plots, it can be concluded that TD Learning does not necessarily perform outstandingly as the Win Rate is 0%. In other words, the agent is unable to complete the game without landing on a mine for all 1000 episodes. The average number of step is 28.98 for TD Learning with the specific parameters. However, the average reward accumulated here is 65.367, which indicates some learning. \n",
    "\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Based on our plots, it is evident that DQN significantly outperformed both TD Learning and Random agents in completing the Minesweeper game. One key reason for this is DQN’s use of experience replay. It stores past experiences as tuples of (state, action, reward, next state) in a dataset called replay memory and randomly samples from this memory during training. As a result, DQN demonstrated improved performance with each consecutive episode, as shown in the Win Rate Over Episodes plot.\n",
    "\n",
    "Additionally, experience replay enabled DQN to maintain a balance between exploration and exploitation. The Exploration Rate plot illustrates how the exploration rate gradually decreased over time, allowing the agent to shift toward exploitation strategies that optimized its chances of winning. This trend is consistent across all plots, demonstrating how DQN’s epsilon-greedy strategy facilitated steady learning and ultimately led to consistent success in Minesweeper.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "One limitation to the random agent is the fact that it does not learn from previous games which isn’t good since there is no room for improvement. The random agents goes into the game blindly with no strategy, making moves based off of luck. Another limitation is the variance in the game. Since the random agent depends on luck, one game can be very short by clicking on the mine within one or two moves. On the other hand there could be a more successful game where the random agent gets really lucky with its moves and avoids the mine for longer.   \n",
    "\n",
    "A key limitation of the TD Learning agent in Minesweeper is that the board environment is partially observable. Since the agent does not have full visibility of the game state, it struggles to accurately evaluate its position and make optimal decisions. TD Learning updates its value estimates based on immediate rewards, which is not ideal in Minesweeper, as success requires a multi-step strategy rather than short-term rewards. Additionally, Minesweeper presents a unique challenge: stepping on a mine results in an automatic loss. This makes it difficult for TD Learning to effectively balance exploration and exploitation.\n",
    "\n",
    "One of the main limitations of the current DQN implementation is the instability in the learning curve, where there are sudden drops in rewards even after periods of learning. This suggests that the agent occasionally fails to generalize its learned strategy and potentially gets trapped in suboptimal policies or overfitting to specific patterns. Additionally, since Minesweeper has inherent uncertainty (e.g., forced guessing when no safe move is available), the agent may struggle in situations where logical deduction is limited, and lead to inconsistent performance across episodes.\n",
    "   \n",
    "\n",
    "\n",
    "### Future work\n",
    "To improve the stability of learning and address the sudden drops in rewards, future work could explore techniques like prioritized experience replay, which ensures the agent learns more effectively from critical experiences rather than sampling past experiences uniformly. Another promising direction is integrating probabilistic reasoning into the model, where the agent can compute the likelihood of a tile being a mine based on nearby revealed numbers. This could be achieved by either modifying the state representation to include probability estimates or by incorporating a hybrid approach that combines reinforcement learning with rule-based decision-making. Ultimately, the goal is to develop a more stable and intelligent Minesweeper solver that can handle uncertainty more effectively while maintaining consistent performance improvements over time.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Minesweeper itself is a pretty ethical game. We will not be using any data that would intrude personal privacy. Our solver will not conflict with any other player’s experience since this is only a single player game. The only concerns that we could possibly have is with our AI methods, where our solution would defeat the purpose of the game and be too overpowered. To prevent any issues, we will make sure to state all the methods that we use and be transparent as possible.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our project demonstrates the significant benefits of utilizing Deep Q-Network (DQN) for training the optimal solution for the Mineaweeper compared to the temporal difference learning (TD learning). Unlike the other two models, DQN uses its neural network and experience replay to generalize across board states and finally find the optimal solution. This project shows the power of deep reinforcement learning in solving complex decision-making problems like Minesweeper. However, our model still needs further improvement to stabilize training and enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name= \"kayenote\"></a>1.[^](#kaye): Kaye, R. (2000). \"Minesweeper is NP-Complete.\" Retrieved February 13, 2025. From https://academic.timwylie.com/17CSCI4341/minesweeper_kay.pdf <br> \n",
    "<a name= \"studholmenote\"></a>2.[^](#studholme):Studholme, C. (2000). Minesweeper as a Constraint Satisfaction Problem. Unpublished project report. Retrieved February 13, 2025. From https://www.cs.toronto.edu/~cvs/minesweeper/minesweeper.pdf<br> \n",
    "<a name= \"qingnote\"></a>3.[^](#qing):Qing, Y. et al. (2020). Critical exponents and the universality class of a minesweeper percolation model. International Journal of Modern Physics C, Volume 31, Issue 9, id. 2050129. Retrieved February 13, 2025. From https://ui.adsabs.harvard.edu/abs/2020IJMPC..3150129Q/abstract DOI: 10.1142/S0129183120501296 <br> \n",
    "<a name= \"tunote\"></a>4.[^](#tu):Tu, J. (n.d.). Exploring Efficient Strategies for Minesweeper. Retrieved February 13, 2025. From https://cdn.aaai.org/ocs/ws/ws0294/15091-68459-1-PB.pdf .<br> \n",
    "<a name= \"smuldersnote\"></a>5.[^](#smulders): Smulders, B.G.J.P.( 25 Jun 2023) Optimizing minesweeper and its hexagonal variant with deep reinforcement learning. Retrieved February 13, 2025. From https://pure.tue.nl/ws/portalfiles/portal/307404348/Thesis_BDS_Juli_G._Smulders.pdf  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
